# Exchange API Rate Limits & Performance Constraints

**Date:** 2026-01-14
**Purpose:** Phase 1 Performance Optimization - Exchange Compatibility Analysis
**Project:** Funding Arbitrage Bot (Lighter + X10)

---

## üìä CRITICAL FINDINGS

### üî¥ LIGHTER EXCHANGE - Rate Limits

**Source:** [Lighter API Documentation - Rate Limits](https://apidocs.lighter.xyz/docs/rate-limits)

**IMPORTANT:** User has **PREMIUM ACCOUNT** - This dramatically changes optimization strategy!

#### REST API Limits

| Account Type | Requests/Minute | Weighted System | Strategy |
|--------------|-----------------|-----------------|----------|
| **Premium (USER)** | **24,000** | ‚úÖ YES (per endpoint weights) | **AGGRESSIVE parallelization!** üöÄ |
| Standard | 60 | ‚ùå NO (hard cap) | Conservative |

**Premium Endpoint Weights:**
- Per User: 6 (very cheap!)
- Per User: 50
- Per User: 100
- Per User: 150
- Per User: 1200
- Other: 300

**IMPLICATION:** With 24,000 weighted requests/minute, we can make **400 requests/second** if all are weight 6!

**Endpoint Weights (Premium):**
- Per User: 6
- Per User: 50
- Per User: 100
- Per User: 150
- Per User: 1200
- Other: 300

#### WebSocket Limits (per IP)

| Resource | Limit | Implications |
|----------|-------|--------------|
| **Connections** | **100** | Max 100 concurrent WS connections |
| **Subscriptions/Connection** | **100** | Max 100 channels per connection |
| **Total Subscriptions** | **1000** | Combined across all connections |
| **Messages/Minute** | **200** | ‚ö†Ô∏è **VERY STRICT** |
| **Inflight Messages** | **50** | Max 50 pending messages |
| **Unique Accounts** | **10** | Max 10 different accounts |

#### Special Transaction Limits (Standard Account)

| Transaction Type | Limit |
|-----------------|-------|
| Default | 40 requests/minute |
| `L2Withdraw` | 2 requests/minute |
| `L2UpdateLeverage` | **1 request/minute** üî¥ |
| `L2CreateSubAccount` | 2 requests/minute |
| `L2Transfer` | **1 request/minute** üî¥ |
| `L2ChangePubKey` | 2 requests/10 seconds |

**Error Response:** HTTP `429 Too Many Requests`
**WebSocket Penalty:** Excessive messages ‚Üí disconnection

---

### üîµ X10 EXTENDED EXCHANGE - Rate Limits

**Source:** [Extended API Documentation](https://api.docs.extended.exchange/)

#### REST API Limits

| Resource | Limit | Scope |
|----------|-------|-------|
| **All REST Endpoints** | **1,000 requests/minute** | Per IP address |
| **Throttling** | IP-based | Shared across all endpoints |

#### WebSocket Limits

‚ö†Ô∏è **Specific WebSocket connection limits not explicitly documented** in the retrieved content.
**Assumption:** Based on industry standards:
- Likely: 5-10 connections per IP
- Likely: 50-100 messages per second
- **RECOMMENDATION:** Test empirically and implement conservative limits

#### Python SDK

**Repository:** [x10xchange/python_sdk](https://github.com/x10xchange/python_sdk)
**Requirement:** Python 3.10+

---

## ‚ö†Ô∏è CRITICAL IMPLICATIONS FOR PHASE 1 OPTIMIZATION

### ‚ùå DANGEROUS OPTIMIZATIONS (Would Violate Limits)

1. **Aggressive Parallel Market Data Refresh**
   - Lighter: 200 messages/min limit = **3.3 messages/second**
   - **10 symbols refreshed simultaneously** = 30+ messages ‚Üí **RATE LIMIT VIOLATION** ‚ö†Ô∏è

2. **Unbounded Concurrent Exit Evaluations**
   - Lighter: 50 inflight messages max
   - **20 concurrent evaluations** = 20+ WS subscriptions ‚Üí **DANGER** ‚ö†Ô∏è

3. **High-Frequency Order Placement**
   - Lighter Standard: 60 requests/minute = **1 request/second**
   - **Attempting 10 orders/second** ‚Üí **429 ERROR** ‚ö†Ô∏è

### ‚úÖ SAFE OPTIMIZATIONS (Within Limits)

1. **Connection Pooling** ‚úÖ
   - Reduces TCP handshake overhead
   - Fewer connections = less resource usage
   - **Benefit:** 20-40% latency reduction
   - **Risk:** NONE (actually helps stay within limits!)

2. **Metadata Caching** ‚úÖ
   - Market info, fee schedules cache with 5-10s TTL
   - **Benefit:** 90% reduction in redundant API calls
   - **Risk:** NONE (reduces load!)

3. **Intelligent Batching** ‚úÖ
   - Batch market data refresh within rate limit budget
   - Example: Refresh 10 symbols over 3 seconds (3-4 symbols/sec)
   - **Benefit:** 30-50% faster without violating limits
   - **Risk:** LOW (if properly throttled)

4. **Request Coalescing** ‚úÖ
   - Combine multiple reads into single bulk operation
   - Example: Get all positions in 1 call instead of 1 per position
   - **Benefit:** Dramatically reduces request count
   - **Risk:** NONE (reduces load!)

---

## üéØ EXCHANGE-COMPLIANT PHASE 1 OPTIMIZATION PLAN

### 1.1 SAFE LATENCY MONITORING ‚úÖ

**Goal:** Measure performance without increasing load

**Implementation:**
```python
# Non-invasive metrics collection
@contextmanager
def track_operation(operation: str):
    start = time.monotonic()
    try:
        yield
    finally:
        duration_ms = (time.monotonic() - start) * 1000
        metrics.record(operation, duration_ms)
```

**Key Metrics to Track:**
- Order placement latency (P50, P95, P99)
- Market data refresh duration
- Exit evaluation duration
- API response time per exchange
- WebSocket message rate
- **Rate limit proximity** (% of limit used)

**Exchange-Specific Considerations:**
- ‚úÖ Track **messages sent per minute** (Lighter: stay under 200)
- ‚úÖ Track **inflight message count** (Lighter: stay under 50)
- ‚úÖ Track **active subscriptions** (Lighter: stay under 1000 total)
- ‚úÖ Track **requests per minute** (X10: stay under 1000)

**Risk:** NONE (observability only)

---

### 1.2 SAFE ASYNC CONCURRENCY ‚úÖ

**Goal:** Parallelize within rate limits

#### Market Data Refresh Optimization

**BEFORE (Sequential):**
```python
for symbol in symbols:  # 10 symbols
    await refresh_symbol(symbol)  # 10 sequential calls
# Total: ~10 seconds
```

**AFTER (Rate-Limited Parallel):**
```python
async def refresh_symbols_batch(symbols: list[str], batch_size: int = 3):
    """Refresh in batches to stay within Lighter's 200 msg/min limit."""
    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i+batch_size]
        await asyncio.gather(*[
            refresh_symbol(symbol) for symbol in batch
        ])
        # Throttle: 3 symbols per second = 180/min (under 200 limit)
        await asyncio.sleep(1.0)
# Total: ~4 seconds (2.5x faster, safe!)
```

**Rate Limit Math:**
- 10 symbols / 3 seconds = **3.33 symbols/second**
- 3.33 symbols/sec √ó 60 sec = **200 symbols/minute** ‚úÖ (at limit!)
- **Safe buffer:** Use batch_size=2 for safety margin

#### Exit Evaluation Parallelization

**BEFORE (Sequential):**
```python
for trade in open_trades:  # 5 trades
    await evaluate_exit(trade)  # 5 sequential evaluations
# Total: ~500ms (100ms per trade)
```

**AFTER (Concurrency-Limited Parallel):**
```python
semaphore = asyncio.Semaphore(3)  # Max 3 concurrent

async def evaluate_exit_with_limit(trade: Trade):
    async with semaphore:
        return await evaluate_exit(trade)

results = await asyncio.gather(*[
    evaluate_exit_with_limit(trade) for trade in open_trades
])
# Total: ~200ms (3 concurrent = 3 batches, ~67ms per batch)
```

**Rate Limit Math:**
- Max 3 concurrent evaluations
- Each evaluation = ~1-2 WebSocket messages
- **Inflight messages:** 3 √ó 2 = **6** ‚úÖ (well under 50!)

**Risk:** MEDIUM (requires careful tuning of batch sizes)

**Mitigation:**
- Start with conservative limits (batch_size=2, semaphore=3)
- Monitor rate limit proximity metrics
- Implement dynamic throttling if approaching limits
- Feature flags for quick rollback

---

### 1.3 SAFE NETWORK OPTIMIZATION ‚úÖ

**Goal:** Reduce latency without increasing request count

#### HTTP Connection Pooling

**Implementation:**
```python
# adapters/exchanges/base_adapter.py
class BaseAdapter:
    def __init__(self):
        self._session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(
                limit=20,              # Max 20 connections
                limit_per_host=10,      # Max 10 per host
                keepalive_timeout=30,   # Reuse connections
                enable_cleanup_closed=True,
            )
        )
```

**Benefits:**
- Eliminates TCP handshake overhead (~20-40ms per request)
- **Fewer connections** = easier to stay within limits!
- Reduces server load

**Risk:** LOW (actually helps with rate limit compliance!)

#### Request Caching (Metadata)

**Implementation:**
```python
class MetadataCache:
    def __init__(self, ttl_seconds: int = 10):
        self._cache: dict[str, tuple[Any, float]] = {}
        self._ttl = ttl_seconds

    async def get_or_fetch(self, key: str, fetch_fn: Callable) -> Any:
        if key in self._cache:
            value, timestamp = self._cache[key]
            if time.monotonic() - timestamp < self._ttl:
                return value  # Cache hit!

        value = await fetch_fn()
        self._cache[key] = (value, time.monotonic())
        return value

# Usage:
market_info = await cache.get_or_fetch(
    f"market_info:{symbol}",
    lambda: fetch_market_info(symbol)  # Only called if cache miss
)
```

**Benefits:**
- **90% cache hit rate** for market metadata
- Reduces API calls by 90%!
- **HUGE help for staying within rate limits**

**Risk:** NONE (cached data has short TTL)

#### WebSocket Message Batching (Conservative)

**Implementation:**
```python
class SafeWebSocketBatcher:
    """Batch WS messages conservatively to stay within 200/min limit."""

    def __init__(self, interval_ms: int = 500):  # 500ms = 2 batches/sec
        self._interval = interval_ms / 1000
        self._buffer: list[dict] = []
        self._last_flush = time.monotonic()

    async def send(self, message: dict):
        self._buffer.append(message)
        now = time.monotonic()
        if now - self._last_flush >= self._interval or len(self._buffer) >= 10:
            await self._flush()
            self._last_flush = now

    async def _flush(self):
        if self._buffer:
            await self._ws.send_json(self._buffer)
            message_count = len(self._buffer)
            self._buffer.clear()
            metrics.record("ws_batch_messages", message_count)
```

**Rate Limit Math:**
- 500ms interval = 2 batches/second
- 2 batches/sec √ó 60 sec = **120 batches/minute** ‚úÖ
- Even if each batch = 10 messages = **1200 messages** ‚ùå TOO HIGH!

**Revised Batching Strategy:**
```python
# Lighter-safe batching: 1 message per 500ms = 120/min
class LighterSafeBatcher:
    def __init__(self, interval_ms: int = 500):
        self._interval = interval_ms / 1000  # 500ms
        self._buffer: list[dict] = []
        self._max_buffer_size = 1  # NO ACTUAL BATCHING for Lighter!
```

**Conclusion:** For Lighter, **message batching is NOT recommended** due to the strict 200/min limit.
**Alternative:** Send messages immediately but throttle the rate.

**Risk:** LOW (conservative approach)

---

### 1.4 EXCHANGE-SPECIFIC SAFE IMPLEMENTATION

#### Lighter-Specific Optimizations

```python
class LighterRateLimitAwareClient:
    """Lighter client that respects strict rate limits."""

    # Rate limits (conservative 90% of actual limits)
    MAX_WS_MESSAGES_PER_MINUTE = 180  # 90% of 200
    MAX_INFLIGHT_MESSAGES = 45         # 90% of 50
    MAX_SUBSCRIPTIONS = 900            # 90% of 1000

    def __init__(self):
        self._message_limiter = RateLimiter(
            max_calls=self.MAX_WS_MESSAGES_PER_MINUTE,
            period=60
        )
        self._inflight_semaphore = asyncio.Semaphore(
            self.MAX_INFLIGHT_MESSAGES
        )

    async def send_ws_message(self, message: dict):
        """Send message with rate limiting."""
        async with self._inflight_semaphore:
            await self._message_limiter.acquire()
            await self._ws.send_json(message)
```

#### X10-Specific Optimizations

```python
class X10RateLimitAwareClient:
    """X10 client with conservative rate limiting."""

    # Rate limits (conservative 90% of actual limits)
    MAX_REQUESTS_PER_MINUTE = 900  # 90% of 1000

    def __init__(self):
        self._request_limiter = RateLimiter(
            max_calls=self.MAX_REQUESTS_PER_MINUTE,
            period=60
        )

    async def make_request(self, endpoint: str, **kwargs):
        """Make REST API request with rate limiting."""
        await self._request_limiter.acquire()
        return await self._session.request(endpoint, **kwargs)
```

---

## üìã PHASE 1 REVISED TIMELINE (Exchange-Compliant)

| Week | Task | Exchange Considerations |
|------|------|-------------------------|
| 1-2 | **Latency Monitoring** | Track rate limit proximity |
| 3-4 | **Safe Async Optimization** | Conservative batch sizes |
| 5 | **Network Optimization** | Connection pooling (safe!) |
| 6-7 | **Real-Time Dashboard** | Show rate limit usage |
| 8 | **Testing & Validation** | Exchange-specific tests |

**Total:** 7-8 weeks (unchanged)

---

## ‚úÖ ACCEPTANCE CRITERIA (REVISED)

Phase 1 is complete when:

- [ ] All latency metrics tracked (including rate limit proximity)
- [ ] Market data refresh <500ms (without violating limits)
- [ ] Exit evaluation <50ms per position (without violating limits)
- [ ] **Rate limit buffer >10%** (never exceed 90% of limits)
- [ ] **Zero 429 errors** (rate limit violations)
- [ ] Zero data loss during high load
- [ ] All 534+ tests still pass
- [ ] Dashboard shows live metrics + rate limit usage

---

## üéØ NEXT STEPS

**Recommended Approach:**

1. **Start with Monitoring (Week 1-2)**
   - Track current rate limit usage
   - Identify headroom
   - Establish baseline metrics

2. **Implement Conservative Optimizations (Week 3-5)**
   - Start with 50% of theoretical maximums
   - Monitor rate limit proximity
   - Gradually increase if safe

3. **Exchange-Specific Testing (Week 6-7)**
   - Test with Lighter testnet first
   - Test with X10 testnet first
   - Measure actual vs. theoretical limits

4. **Conservative Rollout (Week 8)**
   - Feature flags for each optimization
   - Gradual rollout to production
   - Monitor for 429 errors
   - Rollback immediately if issues detected

---

## üìö REFERENCES

- **[Lighter Rate Limits](https://apidocs.lighter.xyz/docs/rate-limits)** - Official rate limit documentation
- **[Lighter WebSocket](https://apidocs.lighter.xyz/docs/websocket-reference)** - WebSocket API reference
- **[X10 Extended API](https://api.docs.extended.exchange/)** - Official API documentation
- **[X10 Python SDK](https://github.com/x10xchange/python_sdk)** - Official Python SDK

---

**Generated:** 2026-01-14
**Author:** Claude Code (SuperClaude Task Agent)
**Status:** Exchange-Compliant Optimization Plan ‚úÖ
